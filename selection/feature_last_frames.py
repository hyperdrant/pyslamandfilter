import numpy as np, array
import os
# # 假设您的数据保存在名为 data_dict 的变量中
# data_dict = {
#     1319:{
#   'Positions': [(697.0, 154.0), (695.0, 155.0), (693.0, 153.0), (690.0, 151.0)],
#   'Descriptors': [np.array([ 63,  95, 118,  78,  53,  54, 222, 240,  85,  78, 255, 230, 107,
#        104, 183, 181, 126, 224, 198, 238, 237, 163, 109, 238,  50, 255,
#        128, 131, 127, 255, 106,  85]), np.array([ 63,  95, 118,  78,  53,  54,  94, 248, 117,  14, 255, 230, 107,
#        104, 183, 181, 126, 226, 134, 238, 236, 167, 109, 239, 178, 255,
#        128, 131, 103, 255, 110, 197]), np.array([ 63,  95, 118,  78, 127, 183, 222, 120, 101, 206, 253, 175, 106,
#        108, 183, 181, 126, 226, 198, 238, 236, 163, 108, 239,  50, 253,
#          0, 155, 127, 255, 110, 213]), np.array([ 63,  95, 118,  78, 119, 183, 222, 124, 101, 206, 253, 175, 107,
#        108, 183, 181, 126, 226, 198, 238, 236, 163,  44, 239,  50, 255,
#          0, 147, 111, 255, 110, 221])],
#   'Semantic Class': [8, 8, 10, 10],
#   'Confidence': [0.42, 0.45, 0.51, 0.52],
#   'Depth': [4, 4, 3, 3],
#   'Observability': 4,
#   'Corner Score': [153.0, 152.0, 152.0, 156.0],
#   'Standard Deviation': 18.7,
#   'Contrast': [0, 0, 0, 0],
#   'image_names': ['0000000006.png','0000000007.png', '0000000008.png', '0000000009.png'],
#   'Robustness': [3.0, 4.0, 3.0, 2.0],
#   'Entropy': [6.7521908292410835, 6.741145256380516, 6.620488105054848, 6.729058368484813],
#   'Geometric': [1.0, 1.0, 1.0, 1.0]
# },
# 1410:
#   {'Positions': [(1045.095, 230.17), (1084.493, 236.39), (1132.186, 244.685), (1186.929, 251.32)],
#   'Descriptors': [np.array([ 48, 129,  95, 239,  22,  66, 111, 121,  84,   0, 243, 226, 115,
#        119, 168, 112, 122, 156,   6, 234,  73, 147,  45,  31, 177, 239,
#        163, 132,  53, 170, 100,  33]), np.array([145, 157,  91, 238,  22, 194,  75, 248,  20,  64, 219, 226,  87,
#         87, 168,  97, 118, 220,  22, 234, 105, 153,  57, 191, 240, 238,
#        163,  68,  60, 139,  96,  57]), np.array([ 21, 149,  91, 233,  52, 135, 106, 252,  20,   9, 219,  70,  85,
#        243, 168,  96, 112, 136,  30, 234, 121, 145,  25, 159, 184, 237,
#        162, 196,  60, 138, 240,  49]), np.array([ 17, 149,  95, 232,  22, 198,  75, 248,  36,  72, 218,  70,  83,
#         87, 184,  96, 112, 196,  30, 238, 105, 153,  25,  63, 241, 173,
#        162, 196,  60, 139, 112,  56])],
#   'Semantic Class': [8, 8, 8, 8],
#   'Confidence': [0.92, 0.98, 0.96, 0.95],
#   'Depth': [84, 92, 103, 110],
#   'Observability': 4,
#   'Corner Score': [50.0, 44.0, 38.0, 38.0],
#   'Standard Deviation': 24.7,
#   'Contrast': [0, 0, 0, 0],
#   'image_names': ['0000000007.png', '0000000008.png', '0000000009.png', '0000000010.png'],
#   'Robustness': [3.0, 2.0, 2.0, 3.0],
#   'Entropy': [6.968842204976724, 6.89801088849259, 6.814729014208125, 6.899569331555351],
#   'Geometric': [0.021457130985747974, 0.8991960143137552, 0.8030908975055184, 0.8015148459286826]}
# }
    
# data_dict = {10:{
#   'Positions': [(768.0, 169.0), (769.0, 170.0), (769.0, 171.0), (769.0, 172.0), (769.2, 172.8), (769.2, 170.4), (769.0, 173.0), (769.2, 170.4), (770.4, 170.4), (770.0, 171.0), (769.2, 170.4), (769.0, 167.0), (767.52, 168.48)],
#   'Descriptors': [np.array([177, 163, 142,  39, 156,  18, 206, 173, 118, 225, 131,  17, 195,
#        104,  82, 161, 216, 239,  72, 245, 252, 235, 221,  14, 168, 239,
#        143,  84, 109,  34, 250, 140]), np.array([ 53, 173, 138,  47, 157, 178, 158, 239, 118, 229, 201, 133, 195,
#        232,  86,   0, 217, 239, 104, 149,  30, 235, 223,   0, 184, 103,
#        143, 125, 205, 102, 251, 198]), np.array([ 47, 169, 230,  62, 205, 178, 148, 107, 122, 215, 233, 149, 201,
#        232, 118, 138, 217, 239, 227, 149,  30, 235, 222, 195,  74, 127,
#         95, 111, 221, 100, 250, 198]), np.array([174, 168, 198,  63, 207,  48, 180, 203, 123, 215, 224, 149, 200,
#        232, 118, 139, 219, 239, 226, 149,  62, 250, 222, 241,  14, 114,
#         95, 111, 221, 100, 251, 198]), np.array([166, 186, 133, 191, 205, 229, 148, 203, 158, 223, 209, 119, 228,
#        236,  82, 139, 221, 107, 227,  21, 183, 110, 219, 177, 140, 114,
#        223, 235, 217, 108, 187, 222]), np.array([162, 186, 135, 191, 207, 231, 180, 207,  30, 223,  81,  87, 200,
#        232, 118, 139, 217, 111, 163, 149, 190, 254, 223, 209,  13, 114,
#        223, 111, 217, 108, 191, 214]), np.array([167, 190, 163,  62, 215, 228, 166, 207,  90,  79,  81, 119, 232,
#        232,  63, 140, 185, 107, 130, 135, 183, 246, 158, 251,  56,  54,
#        223, 170, 205, 124, 247, 215]), np.array([ 34, 186, 167, 191, 205, 199, 180, 239,  95, 223, 209,  87, 196,
#        232, 118, 137, 217, 239, 163, 149, 191, 254, 223, 209, 137, 114,
#        223, 111, 205, 108, 189, 214]), np.array([ 19, 162, 134, 191, 151, 171, 160, 207, 127, 247,  90, 215, 237,
#        168, 114, 136, 249,  15, 163, 149, 150, 252,  31, 213, 140,  86,
#         95,  71, 205,  76, 185, 206]), np.array([ 25, 160, 246,  63, 207, 147, 176, 207, 127, 247, 112,  23, 104,
#        248, 126, 170, 251, 203, 162, 149, 246, 254, 159, 241,  30,  75,
#         87, 103, 237,  68, 235, 142]), np.array([ 19, 160, 134,  55, 215, 187, 176, 207, 127, 243,  90,  87, 108,
#        168, 118, 136, 251, 207, 162, 149, 150, 252,  31, 213,  12,  86,
#         95,  71, 205,  76, 185, 206]), np.array([  1, 168, 254,  57,  79, 137, 176, 207, 255, 247, 116,  23,  40,
#        232, 126,  10, 235,  75,  98, 149, 134, 127, 159, 241,  94,  91,
#         87, 107, 253,   4, 235, 206]), np.array([ 19, 178, 134, 191, 157, 247, 160,  79, 247, 253, 203,  87, 228,
#        232, 114, 196, 233, 175, 234, 149, 151, 255,  31, 215, 157,  86,
#         31,  79, 201,  76, 185, 222])],
#   'Semantic Class': [2, 2, 2, 2, 2, 8, 2, 5, 2, 8, 8, 8, 8],
#   'Confidence': [0.75, 0.56, 0.58, 0.54, 0.29, 0.47, 0.42, 0.36, 0.48, 0.72, 0.66, 0.67, 0.6],
#   'Depth': [15, 14, 14, 15, 15, 15, 15, 15, 16, 17, 16, 16, 16],
#   'Observability': 13,
#   'Corner Score': [148.0, 111.0, 133.0, 117.0, 80.0, 89.0, 115.0, 89.0, 106.0, 102.0, 133.0, 135.0, 133.0],
#   'Standard Deviation': 47.8,
#   'Contrast': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
#   'image_names': ['0', '1', '2', '3', '4', '5', '6', '6', '7', '8', '9', '10', '10'],
#   'Robustness': [2.0, 1.0, 2.0, 2.0, 0.0, 2.0, 2.0, 2.0, 1.0, 4.0, 3.0, 1.0, 2.0],
#   'Entropy': [7.303509036383815, 7.386378944461827, 7.3421018237980995, 7.301790591483423, 7.237981353258114, 7.223145491413984, 7.348737475590127, 7.313466605215671, 7.408558137229052, 7.322547653787215, 7.210931111740964, 7.246669606984141, 7.248021312751746],
#   'Geometric': [1.0, 1.0, 1.0, 1.0, 0.6235693630767305, 0.5561534717932695, 1.0, 0.8696163614978527, 0.5052283981573105, 0.7, 1.0, 1.0, 1.0]}}
# image_id = 10
# valid_image_names = [str(image_id - i) for i in range(2)]  # ['10', '9', '8', '7', '6']

# filtered_data = {}

# image_id = 10

# number_of_frames = 3
# filtered_data = {}
def preserve_last_frames(data_dict,image_id,number_of_frames):
    
    filtered_data = {}
    image_id_str = image_id.split('.')[0]  # 提取文件名中的数字部分
    image_id = int(image_id_str)  # 将数字部分转换为整数
    number_of_frames = 4  # 你想生成的文件数量
    original_length = len(image_id_str)  # 获取数字部分的长度

    # 生成 valid_image_names，并过滤掉小于零的情况
    valid_image_names = [f"{str(image_id - i).zfill(original_length)}.png" for i in range(number_of_frames) if image_id - i >= 0]
    #print(image_id)
    # valid_image_names = [f"{str(image_id - i).zfill(original_length)}.png" for i in range(number_of_frames)]
    # valid_image_names = [str(image_id - i) for i in range(number_of_frames)]  # ['10', '9', '8', '7', '6']
    required_images = {f"{str(image_id).zfill(original_length)}.png", f"{str(image_id - 1).zfill(original_length)}.png"}
    # required_images = {str(image_id), str(image_id - 1)}  # {'10', '9'}
    for key, data_item in data_dict.items():
        image_names = data_item['image_names']
        # 检查是否包含 image_id 和 image_id - 1
        if required_images.issubset(image_names):
            # 创建保留的索引列表
            indices_to_keep = [i for i, name in enumerate(image_names) if name in valid_image_names]
            if indices_to_keep:
                # 过滤 image_names
                filtered_image_names = [data_item['image_names'][i] for i in indices_to_keep]
                # 过滤其他字段
                filtered_data_item = {}
                for field in data_item:
                    value = data_item[field]
                    if isinstance(value, list) and len(value) == len(image_names):
                        filtered_data_item[field] = [value[i] for i in indices_to_keep]
                    else:
                        filtered_data_item[field] = value
                filtered_data_item['image_names'] = filtered_image_names
                # 过滤后再次检查
                if required_images.issubset(filtered_data_item['image_names']):
                    filtered_data[key] = filtered_data_item
            else:
                continue
        else:
            continue
    return filtered_data
# filtered = preserve_last_frames(data_dict,image_id='0000000010.png',number_of_frames=5)
# print(filtered)
# 现在，filtered_data 包含了按照要求筛选后的数据


